{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LORA\n",
    "* Model: gpt2\n",
    "* Evaluation approach: accuracy using the trainer.evaluate()\n",
    "* Fine-tuning dataset: financial_phrasebank - sentences_allagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e2a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -q peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caced957",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2' #'bert-base-uncased'#\n",
    "dataset = \"financial_phrasebank\"\n",
    "subset = 'sentences_allagree'\n",
    "\n",
    "padding_token = '[PAD]'\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c61aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "per_device_train_batch_size = torch.cuda.device_count()\n",
    "if per_device_train_batch_size == 0:\n",
    "    per_device_train_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249d3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417982d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 1358\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 272\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "ds = load_dataset(path = dataset, name = subset, split='train').shuffle(seed = 42)\n",
    "ds = ds.train_test_split(test_size = 0.4, stratify_by_column = 'label', seed = 42)\n",
    "temp = ds['test'].train_test_split(test_size = 0.3, stratify_by_column = 'label', seed = 4)\n",
    "\n",
    "ds['val'] = temp['train']\n",
    "ds['test'] = temp['test']\n",
    "ds = ds.with_format(\"torch\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "names = ds['train'].features['label'].names\n",
    "id2label = {index: item for index, item in enumerate(names)}\n",
    "label2id = {item: index for index, item in enumerate(names)}\n",
    "\n",
    "# load model\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=len(names),\n",
    "\n",
    ").to(device)\n",
    "\n",
    "\n",
    "for param in foundation_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = tokenizer.model_max_length\n",
    "\n",
    "if model_name == 'gpt2':\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    foundation_model.config.pad_token_id =  tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5176b07f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \n",
    "    outputs = tokenizer(examples[\"sentence\"], padding=\"max_length\",\\\n",
    "                                          truncation=True, return_tensors='pt',\\\n",
    "                                        max_length = max_length)\n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = ds[split].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['sentence']\n",
    "      )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7b1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e9f1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6a4542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333432674408"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokenized_dataset['test']['attention_mask'][:30].to(device)\n",
    "input_ids = tokenized_dataset['test']['input_ids'][:30].to(device)\n",
    "labels = tokenized_dataset['test']['label'][:30].to(device)\n",
    "with torch.no_grad():\n",
    "    output = foundation_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "torch.mean(output.logits.argmax(dim=1).eq(labels).to(torch.float32)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c630d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del attention_mask\n",
    "del input_ids\n",
    "del labels\n",
    "del output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c550f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(foundation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "336d3eb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66,816 || all params: 124,508,928 || trainable%: 0.05366362161595352\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "if model_name == 'gpt2':\n",
    "    target_modules=[\"c_proj\"]\n",
    "elif model_name == 'bert-base-uncased':\n",
    "    target_modules=[\"query\",\"key\",\"value\"]\n",
    "\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=16, \n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "from peft import get_peft_model\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc0c9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./peft_project\", \n",
    "                                  logging_steps = 10,\n",
    "                                  num_train_epochs=epochs, \n",
    "                                  learning_rate= 2e-3,\n",
    "                                  per_device_train_batch_size = per_device_train_batch_size, \n",
    "                                  per_device_eval_batch_size = per_device_train_batch_size, \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy='epoch',\n",
    "                                  weight_decay=0.01,\n",
    "                                  load_best_model_at_end=True)\n",
    "\n",
    "trainer = Trainer(model=peft_model, \n",
    "                  args=training_args, \n",
    "                  train_dataset = tokenized_dataset['train'],                 # train_tokenized_datasets, \n",
    "                  eval_dataset =  tokenized_dataset['val'],                # val_tokenized_datasets,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "681b5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1268' max='634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [634/634 06:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training evaluation results: {'eval_loss': 3.422431707382202, 'eval_accuracy': 0.250788643533123, 'eval_runtime': 57.616, 'eval_samples_per_second': 11.004, 'eval_steps_per_second': 11.004}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating before training...\")\n",
    "print(\"Pre-training evaluation results:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6790' max='6790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6790/6790 29:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.287900</td>\n",
       "      <td>1.387829</td>\n",
       "      <td>0.599369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.145800</td>\n",
       "      <td>1.503470</td>\n",
       "      <td>0.747634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>1.330181</td>\n",
       "      <td>0.752366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.963500</td>\n",
       "      <td>0.763407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.291300</td>\n",
       "      <td>0.947129</td>\n",
       "      <td>0.763407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6790, training_loss=1.2581720078777898, metrics={'train_runtime': 1782.1202, 'train_samples_per_second': 3.81, 'train_steps_per_second': 3.81, 'total_flos': 3551229326131200.0, 'train_loss': 1.2581720078777898, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='634' max='634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [634/634 00:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training evaluation results: {'eval_loss': 0.9471291303634644, 'eval_accuracy': 0.7634069400630915, 'eval_runtime': 57.4061, 'eval_samples_per_second': 11.044, 'eval_steps_per_second': 11.044, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating after training...\")\n",
    "print(\"Post-training evaluation results:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "filename = f\"{model_name}.-LORA.-{date.today()}\"\n",
    "# trainer.save_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a18e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds\n",
    "del peft_model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b37c6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7333333492279053"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "# model = AutoPeftModelForSequenceClassification.from_pretrained(filename)\n",
    "\n",
    "NUM_LABELS = len(names)\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(filename, num_labels=NUM_LABELS,\\\n",
    "                                                                    ignore_mismatched_sizes=True).to(device)\n",
    "\n",
    "if model_name == 'gpt2':\n",
    "    lora_model.config.pad_token_id =  tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "attention_mask = tokenized_dataset['test']['attention_mask'][:30].to(device)\n",
    "input_ids = tokenized_dataset['test']['input_ids'][:30].to(device)\n",
    "labels = tokenized_dataset['test']['label'][:30].to(device)\n",
    "with torch.no_grad():\n",
    "    output = lora_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "torch.mean(output.logits.argmax(dim=1).eq(labels).to(torch.float32)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6f6b4",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63cec457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "\n",
    "if model_name == 'gpt2':\n",
    "    target_modules=[\"c_proj\"]\n",
    "elif model_name == 'bert-base-uncased':\n",
    "    target_modules=[\"query\",\"key\",\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd197b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoftQConfig\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=len(names),\n",
    "    load_in_4bit=True,\n",
    "\n",
    "    device_map=\"auto\")\n",
    "\n",
    "if model_name == 'gpt2':\n",
    "    model.config.pad_token_id =  tokenizer.pad_token_id\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "loftq_config = LoftQConfig(loftq_bits=4)\n",
    "config = LoraConfig(\n",
    "\n",
    "    r=1, \n",
    "    lora_alpha=32, \n",
    "    target_modules = target_modules, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model,8)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a501a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94762202",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        per_device_train_batch_size= per_device_train_batch_size,\n",
    "        per_device_eval_batch_size = per_device_train_batch_size,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy='epoch',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        num_train_epochs=epochs,\n",
    "\n",
    "    metric_for_best_model='accuracy',\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset = tokenized_dataset['train'], \n",
    "    eval_dataset =  tokenized_dataset['val'], \n",
    "    args= args,\n",
    "    data_collator= DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e1bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1268' max='634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [634/634 06:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training evaluation results: {'eval_loss': 3.9301881790161133, 'eval_accuracy': 0.250788643533123, 'eval_runtime': 40.997, 'eval_samples_per_second': 15.465, 'eval_steps_per_second': 15.465}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating before training...\")\n",
    "print(\"Pre-training evaluation results:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e571474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2716' max='2716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2716/2716 12:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437165</td>\n",
       "      <td>0.856467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400139</td>\n",
       "      <td>0.949527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2716, training_loss=0.8303643656409015, metrics={'train_runtime': 724.1975, 'train_samples_per_second': 3.75, 'train_steps_per_second': 3.75, 'total_flos': 711835011514368.0, 'train_loss': 0.8303643656409015, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b03db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after training...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training evaluation results: {'eval_loss': 0.40013912320137024, 'eval_accuracy': 0.9495268138801262, 'eval_runtime': 40.0478, 'eval_samples_per_second': 15.831, 'eval_steps_per_second': 15.831, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating after training...\")\n",
    "print(\"Post-training evaluation results:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "568c2845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.47817572951316833,\n",
       " 'test_accuracy': 0.9227941176470589,\n",
       " 'test_runtime': 17.6046,\n",
       " 'test_samples_per_second': 15.451,\n",
       " 'test_steps_per_second': 15.451}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict,label_ids,metrics = trainer.predict(tokenized_dataset['test'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b676b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
